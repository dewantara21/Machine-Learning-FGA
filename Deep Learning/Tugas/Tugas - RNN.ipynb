{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of ML0120EN-3.4-Review-LSTM-CharacterModelling.ipynb","version":"0.3.2","provenance":[{"file_id":"1xd_s-whCtgCvlR7YJp_wlhQAHELbAbts","timestamp":1565462322880}],"collapsed_sections":["_mfGeY9FbNuo","A4GStvvHbNup","kcLkGGUSbNvO"]},"kernelspec":{"name":"python2","display_name":"Python 2"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"PACW5MnobNuJ","colab_type":"text"},"source":["<img src = \"https://i.imgur.com/UjutVJd.jpg\" align = \"center\">\n","\n","\n","# Text generation using RNN/LSTM (Character-level)\n","In this notebook you will learn the How to use TensorFlow for create a Recurrent Neural Network<br />"]},{"cell_type":"markdown","metadata":{"id":"Tdq3corNb22b","colab_type":"text"},"source":["# Table of contents\n","\n","<div>\n","- <a href=\"#intro\">Introduction</a><br />\n","- <a href=\"#arch\">Architectures</a><br />\n","- <a href=\"#lstm\">Long Short-Term Memory Model (LSTM)</a><br />\n","- <a href=\"#build\">Building a LSTM with TensorFlow</a>\n","</div>\n","\n","<hr>"]},{"cell_type":"markdown","metadata":{"id":"pjk6jKSMbNuK","colab_type":"text"},"source":["This code implements a Recurrent Neural Network with LSTM/RNN units for training/sampling from character-level language models. In other words, the model takes a text file as input and trains the RNN network that learns to predict the next character in a sequence.  \n","The RNN can then be used to generate text character by character that will look like the original training data. \n","\n","This code is based on this [blog](http://karpathy.github.io/2015/05/21/rnn-effectiveness/), and the code is an step-by-step implimentation of the [character-level implimentation](https://github.com/crazydonkey200/tensorflow-char-rnn).\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"3x07SIHMbNuL","colab_type":"text"},"source":["First, import the requiered libraries:"]},{"cell_type":"code","metadata":{"id":"ZPFEVJ75bNuL","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","import time\n","import codecs\n","import os\n","import collections\n","from six.moves import cPickle\n","import numpy as np\n","#from tensorflow.python.ops import rnn_cell\n","#from tensorflow.python.ops import seq2seq"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OnA-A6vga9yA","colab_type":"code","colab":{"resources":{"http://localhost:8080/nbextensions/google.colab/files.js":{"data":"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","ok":true,"headers":[["content-type","application/javascript"]],"status":200,"status_text":"OK"}},"base_uri":"https://localhost:8080/","height":77},"outputId":"7ca71680-e413-4e5a-91ff-4a7ee60d6f22","executionInfo":{"status":"ok","timestamp":1565462022819,"user_tz":-420,"elapsed":10299,"user":{"displayName":"A.M Dirham Dewantara","photoUrl":"https://lh5.googleusercontent.com/-WTvjmG5tB8E/AAAAAAAAAAI/AAAAAAAAAPs/wjM4C_zLOm8/s64/photo.jpg","userId":"02029333175577307706"}}},"source":["from google.colab import files\n","uploaded = files.upload()"],"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-f5d41f32-9be4-4ec0-aad6-c891239c3c37\" name=\"files[]\" multiple disabled />\n","     <output id=\"result-f5d41f32-9be4-4ec0-aad6-c891239c3c37\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script src=\"/nbextensions/google.colab/files.js\"></script> "]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Saving indo.txt to indo.txt\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"vN28hSr6bNuO","colab_type":"text"},"source":["### Data loader\n","The following cell is a class that help to read data from input file."]},{"cell_type":"code","metadata":{"id":"sY_Z7U3fbNuP","colab_type":"code","colab":{}},"source":["class TextLoader():\n","    def __init__(self, data_dir, batch_size, seq_length, encoding='utf-8'):\n","        self.data_dir = data_dir\n","        self.batch_size = batch_size\n","        self.seq_length = seq_length\n","        self.encoding = encoding\n","\n","        input_file = os.path.join(data_dir, \"indo.txt\")\n","        vocab_file = os.path.join(data_dir, \"vocab.pkl\")\n","        tensor_file = os.path.join(data_dir, \"data.npy\")\n","\n","        if not (os.path.exists(vocab_file) and os.path.exists(tensor_file)):\n","            print(\"reading text file\")\n","            self.preprocess(input_file, vocab_file, tensor_file)\n","        else:\n","            print(\"loading preprocessed files\")\n","            self.load_preprocessed(vocab_file, tensor_file)\n","        self.create_batches()\n","        self.reset_batch_pointer()\n","\n","    def preprocess(self, input_file, vocab_file, tensor_file):\n","        with codecs.open(input_file, \"r\", encoding=self.encoding) as f:\n","            data = f.read()\n","        counter = collections.Counter(data)\n","        count_pairs = sorted(counter.items(), key=lambda x: -x[1])\n","        self.chars, _ = zip(*count_pairs)\n","        self.vocab_size = len(self.chars)\n","        self.vocab = dict(zip(self.chars, range(len(self.chars))))\n","        with open(vocab_file, 'wb') as f:\n","            cPickle.dump(self.chars, f)\n","        self.tensor = np.array(list(map(self.vocab.get, data)))\n","        np.save(tensor_file, self.tensor)\n","\n","    def load_preprocessed(self, vocab_file, tensor_file):\n","        with open(vocab_file, 'rb') as f:\n","            self.chars = cPickle.load(f)\n","        self.vocab_size = len(self.chars)\n","        self.vocab = dict(zip(self.chars, range(len(self.chars))))\n","        self.tensor = np.load(tensor_file)\n","        self.num_batches = int(self.tensor.size / (self.batch_size * self.seq_length))\n","\n","    def create_batches(self):\n","        self.num_batches = int(self.tensor.size / (self.batch_size * self.seq_length))\n","\n","        # When the data (tensor) is too small, let's give them a better error message\n","        if self.num_batches==0:\n","            assert False, \"Not enough data. Make seq_length and batch_size small.\"\n","\n","        self.tensor = self.tensor[:self.num_batches * self.batch_size * self.seq_length]\n","        xdata = self.tensor\n","        ydata = np.copy(self.tensor)\n","        ydata[:-1] = xdata[1:]\n","        ydata[-1] = xdata[0]\n","        self.x_batches = np.split(xdata.reshape(self.batch_size, -1), self.num_batches, 1)\n","        self.y_batches = np.split(ydata.reshape(self.batch_size, -1), self.num_batches, 1)\n","\n","\n","    def next_batch(self):\n","        x, y = self.x_batches[self.pointer], self.y_batches[self.pointer]\n","        self.pointer += 1\n","        return x, y\n","\n","    def reset_batch_pointer(self):\n","        self.pointer = 0"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"btfca01hbNuR","colab_type":"text"},"source":["### Parameters\n","#### Batch, number_of_batch, batch_size and seq_length\n","what is batch, number_of_batch, batch_size and seq_length in the charcter level example?  \n","\n","Lets assume the input is this sentence: '__here is an example__'. Then:\n","- txt_length = 18  \n","- seq_length = 3  \n","- batch_size = 2  \n","- number_of_batchs = 18/3*2 = 3\n","- batch = array (['h','e','r'],['e',' ','i'])\n","- sample Seq = 'her'  "]},{"cell_type":"markdown","metadata":{"id":"CwJUzlwTbNuS","colab_type":"text"},"source":["Ok, now, lets look at a real dataset, with real parameters. "]},{"cell_type":"code","metadata":{"id":"gVBg_fWRbNuT","colab_type":"code","colab":{}},"source":["seq_length = 50 # RNN sequence length\n","batch_size = 60  # minibatch size, i.e. size of data in each epoch\n","num_epochs = 125 # you should change it to 50 if you want to see a relatively good results\n","learning_rate = 0.002\n","decay_rate = 0.97\n","rnn_size = 128 # size of RNN hidden state (output dimension)\n","num_layers = 2 #number of layers in the RNN"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z7qy5i-YbNuV","colab_type":"text"},"source":["We download the input file, and print a part of it:"]},{"cell_type":"code","metadata":{"id":"sNiPhNiybNuW","colab_type":"code","outputId":"06defc01-3a57-4773-90e8-02054008d561","executionInfo":{"status":"ok","timestamp":1565462022833,"user_tz":-420,"elapsed":10260,"user":{"displayName":"A.M Dirham Dewantara","photoUrl":"https://lh5.googleusercontent.com/-WTvjmG5tB8E/AAAAAAAAAAI/AAAAAAAAAPs/wjM4C_zLOm8/s64/photo.jpg","userId":"02029333175577307706"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["from google.colab import files\n","with open('indo.txt', 'r') as f:\n","    read_data = f.read()\n","    print read_data[0:10]\n","f.closed"],"execution_count":5,"outputs":[{"output_type":"stream","text":["﻿saya me\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"tOO2MUdEbNuY","colab_type":"text"},"source":["Now, we can read the data at batches using the __TextLoader__ class. It will convert the characters to numbers, and represent each sequence as a vector in batches:"]},{"cell_type":"code","metadata":{"id":"bBsrCGq_bNuZ","colab_type":"code","outputId":"ec8f8896-1c40-4158-ae4f-aa801e759bdd","executionInfo":{"status":"ok","timestamp":1565462022837,"user_tz":-420,"elapsed":10243,"user":{"displayName":"A.M Dirham Dewantara","photoUrl":"https://lh5.googleusercontent.com/-WTvjmG5tB8E/AAAAAAAAAAI/AAAAAAAAAPs/wjM4C_zLOm8/s64/photo.jpg","userId":"02029333175577307706"}},"colab":{"base_uri":"https://localhost:8080/","height":243}},"source":["data_loader = TextLoader('', batch_size, seq_length)\n","vocab_size = data_loader.vocab_size\n","print \"vocabulary size:\" ,data_loader.vocab_size\n","print \"Characte8rs:\" ,data_loader.chars\n","print \"vocab number of 'F':\",data_loader.vocab['F']\n","print \"Character sequences (first batch):\", data_loader.x_batches[0]"],"execution_count":6,"outputs":[{"output_type":"stream","text":["reading text file\n","vocabulary size: 65\n","Characte8rs: (u'a', u' ', u'n', u'e', u'i', u't', u'k', u'u', u'm', u'r', u's', u'g', u'd', u'l', u'y', u'b', u'o', u'h', u'p', u'j', u',', u'c', u'.', u'P', u'K', u'S', u'w', u'-', u'H', u'\\n', u'\\r', u'M', u'B', u'(', u')', u'A', u'J', u'R', u'W', u':', u'D', u'L', u'!', u'C', u'T', u'f', u'G', u'1', u'F', u'N', u'E', u'I', u'v', u'3', u'2', u'9', u'?', u'Y', u'z', u'/', u'5', u'6', u'U', u'x', u'\\ufeff')\n","vocab number of 'F': 48\n","Character sequences (first batch): [[64 10  0 ... 10  3  4]\n"," [23  1 12 ...  7  6  1]\n"," [ 0  2  1 ...  0 14  0]\n"," ...\n"," [ 1 10  3 ...  2  1  5]\n"," [ 3 12  0 ...  5  2 14]\n"," [ 7  5  1 ...  4 12  0]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"fZngrIQFbNub","colab_type":"text"},"source":["### Input and output"]},{"cell_type":"code","metadata":{"id":"tqvVzqvDbNuc","colab_type":"code","outputId":"33051376-efb3-41b5-a3df-f586aff8cc65","executionInfo":{"status":"ok","timestamp":1565462022839,"user_tz":-420,"elapsed":10221,"user":{"displayName":"A.M Dirham Dewantara","photoUrl":"https://lh5.googleusercontent.com/-WTvjmG5tB8E/AAAAAAAAAAI/AAAAAAAAAPs/wjM4C_zLOm8/s64/photo.jpg","userId":"02029333175577307706"}},"colab":{"base_uri":"https://localhost:8080/","height":148}},"source":["x,y = data_loader.next_batch()\n","x"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[64, 10,  0, ..., 10,  3,  4],\n","       [23,  1, 12, ...,  7,  6,  1],\n","       [ 0,  2,  1, ...,  0, 14,  0],\n","       ...,\n","       [ 1, 10,  3, ...,  2,  1,  5],\n","       [ 3, 12,  0, ...,  5,  2, 14],\n","       [ 7,  5,  1, ...,  4, 12,  0]])"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"LYSD2grBbNue","colab_type":"code","outputId":"3edf9695-db6f-4264-b0c8-bd8daadd80f4","executionInfo":{"status":"ok","timestamp":1565462022840,"user_tz":-420,"elapsed":10204,"user":{"displayName":"A.M Dirham Dewantara","photoUrl":"https://lh5.googleusercontent.com/-WTvjmG5tB8E/AAAAAAAAAAI/AAAAAAAAAPs/wjM4C_zLOm8/s64/photo.jpg","userId":"02029333175577307706"}},"colab":{"base_uri":"https://localhost:8080/","height":36}},"source":["x.shape  #batch_size =60, seq_length=50"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(60, 50)"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"VbTPCU5vbNui","colab_type":"text"},"source":["Here, __y__ is the next character for each character in __x__:"]},{"cell_type":"code","metadata":{"id":"rhmI98PkbNuj","colab_type":"code","outputId":"2ccbec72-baee-4ae5-e249-83a1d331fb7b","executionInfo":{"status":"ok","timestamp":1565462022842,"user_tz":-420,"elapsed":10186,"user":{"displayName":"A.M Dirham Dewantara","photoUrl":"https://lh5.googleusercontent.com/-WTvjmG5tB8E/AAAAAAAAAAI/AAAAAAAAAPs/wjM4C_zLOm8/s64/photo.jpg","userId":"02029333175577307706"}},"colab":{"base_uri":"https://localhost:8080/","height":148}},"source":["y"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[10,  0, 14, ...,  3,  4,  2],\n","       [ 1, 12,  4, ...,  6,  1,  8],\n","       [ 2,  1, 18, ..., 14,  0,  1],\n","       ...,\n","       [10,  3,  9, ...,  1,  5,  3],\n","       [12,  0, 22, ...,  2, 14,  0],\n","       [ 5,  1,  6, ..., 12,  0,  6]])"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"_mfGeY9FbNuo","colab_type":"text"},"source":["### LSTM Architecture\n","Each LSTM cell has 5 parts:\n","1. Input\n","2. prv_state\n","3. prv_output\n","4. new_state\n","5. new_output\n","\n","\n","- Each LSTM cell has an input layre, which its size is 128 units in our case. The input vector's dimension also is 128, which is the dimensionality of embedding vector, so called, dimension size of W2V/embedding, for each character/word.\n","- Each LSTM cell has a hidden layer, where there are some hidden units. The argument n_hidden=128 of BasicLSTMCell is the number of hidden units of the LSTM (inside A). It keeps the size of the output and state vector. It is also known as, rnn_size, num_units, num_hidden_units, and LSTM size\n","- An LSTM keeps two pieces of information as it propagates through time: \n","    - __hidden state__ vector: Each LSTM cell accept a vector, called __hidden state__ vector, of size n_hidden=128, and its value is returned to the LSTM cell in the next step. The __hidden state__ vector; which is the memory of the LSTM, accumulates using its (forget, input, and output) gates through time. \"num_units\" is equivalant to \"size of RNN hidden state\". number of hidden units is the dimensianality of the output (= dimesianality of the state) of the LSTM cell.\n","    - __previous time-step output__: For each LSTM cell that we initialize, we need to supply a value (128 in this case) for the hidden dimension, or as some people like to call it, the number of units in the LSTM cell. \n","\n","\n","#### num_layers = 2 \n","- number of layers in the RNN, is defined by num_layers\n","- An input of MultiRNNCell is __cells__ which is list of RNNCells that will be composed in this order."]},{"cell_type":"markdown","metadata":{"id":"A4GStvvHbNup","colab_type":"text"},"source":["### Defining stacked RNN Cell"]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"lMHoAW3abNuq","colab_type":"text"},"source":["__BasicRNNCell__ is the most basic RNN cell."]},{"cell_type":"code","metadata":{"id":"WJMSYRYLbNur","colab_type":"code","outputId":"ee6063fb-3346-46de-c69e-df58688405b7","executionInfo":{"status":"ok","timestamp":1565462025609,"user_tz":-420,"elapsed":12934,"user":{"displayName":"A.M Dirham Dewantara","photoUrl":"https://lh5.googleusercontent.com/-WTvjmG5tB8E/AAAAAAAAAAI/AAAAAAAAAPs/wjM4C_zLOm8/s64/photo.jpg","userId":"02029333175577307706"}},"colab":{"base_uri":"https://localhost:8080/","height":262}},"source":["cell = tf.contrib.rnn.BasicRNNCell(rnn_size)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["WARNING: Logging before flag parsing goes to stderr.\n","W0810 18:33:34.262375 139916882519936 lazy_loader.py:50] \n","The TensorFlow contrib module will not be included in TensorFlow 2.0.\n","For more information, please see:\n","  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n","  * https://github.com/tensorflow/addons\n","  * https://github.com/tensorflow/io (for I/O related ops)\n","If you depend on functionality not listed there, please file an issue.\n","\n","W0810 18:33:34.264131 139916882519936 deprecation.py:323] From <ipython-input-10-cbc7d8d66937>:1: __init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","This class is equivalent as tf.keras.layers.SimpleRNNCell, and will be replaced by that in Tensorflow 2.0.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"wChFLvsHbNut","colab_type":"code","outputId":"2f8b2d6a-01a5-42c8-98aa-a611e0995af5","executionInfo":{"status":"ok","timestamp":1565462025612,"user_tz":-420,"elapsed":12915,"user":{"displayName":"A.M Dirham Dewantara","photoUrl":"https://lh5.googleusercontent.com/-WTvjmG5tB8E/AAAAAAAAAAI/AAAAAAAAAPs/wjM4C_zLOm8/s64/photo.jpg","userId":"02029333175577307706"}},"colab":{"base_uri":"https://localhost:8080/","height":112}},"source":["# a two layer cell\n","stacked_cell = tf.contrib.rnn.MultiRNNCell([cell] * num_layers)"],"execution_count":11,"outputs":[{"output_type":"stream","text":["W0810 18:33:34.279531 139916882519936 deprecation.py:323] From <ipython-input-11-32025279f672>:1: __init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n","W0810 18:33:34.281141 139916882519936 rnn_cell_impl.py:1642] At least two cells provided to MultiRNNCell are the same object and will share weights.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"B8nlpbtYbNuv","colab_type":"code","outputId":"f264921c-d846-43a5-b067-3dd2c3fe74a3","executionInfo":{"status":"ok","timestamp":1565462025615,"user_tz":-420,"elapsed":12896,"user":{"displayName":"A.M Dirham Dewantara","photoUrl":"https://lh5.googleusercontent.com/-WTvjmG5tB8E/AAAAAAAAAAI/AAAAAAAAAPs/wjM4C_zLOm8/s64/photo.jpg","userId":"02029333175577307706"}},"colab":{"base_uri":"https://localhost:8080/","height":36}},"source":["# hidden state size\n","stacked_cell.output_size"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["128"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"markdown","metadata":{"id":"H6mVeEigbNux","colab_type":"text"},"source":["__state__ varibale keeps output and new_state of the LSTM, so it is a touple of size:"]},{"cell_type":"code","metadata":{"id":"7EBNSEK6bNuy","colab_type":"code","outputId":"cefb8ccc-17b9-4ab9-b215-dfa2190edf85","executionInfo":{"status":"ok","timestamp":1565462025619,"user_tz":-420,"elapsed":12884,"user":{"displayName":"A.M Dirham Dewantara","photoUrl":"https://lh5.googleusercontent.com/-WTvjmG5tB8E/AAAAAAAAAAI/AAAAAAAAAPs/wjM4C_zLOm8/s64/photo.jpg","userId":"02029333175577307706"}},"colab":{"base_uri":"https://localhost:8080/","height":36}},"source":["stacked_cell.state_size"],"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(128, 128)"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"markdown","metadata":{"id":"EllpfaTybNu0","colab_type":"text"},"source":["Lets define input data:"]},{"cell_type":"code","metadata":{"id":"K-YsOIBebNu1","colab_type":"code","outputId":"8db42c18-e69e-4b57-f797-ebd7e795906b","executionInfo":{"status":"ok","timestamp":1565462025620,"user_tz":-420,"elapsed":12867,"user":{"displayName":"A.M Dirham Dewantara","photoUrl":"https://lh5.googleusercontent.com/-WTvjmG5tB8E/AAAAAAAAAAI/AAAAAAAAAPs/wjM4C_zLOm8/s64/photo.jpg","userId":"02029333175577307706"}},"colab":{"base_uri":"https://localhost:8080/","height":36}},"source":["input_data = tf.placeholder(tf.int32, [batch_size, seq_length])# a 60x50\n","input_data"],"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor 'Placeholder:0' shape=(60, 50) dtype=int32>"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"markdown","metadata":{"id":"wp5_pFi1bNu4","colab_type":"text"},"source":["and target data:"]},{"cell_type":"code","metadata":{"id":"aMZ3xkAsbNu6","colab_type":"code","outputId":"72d6c5d2-a746-4989-ba30-68410a439e42","executionInfo":{"status":"ok","timestamp":1565462025622,"user_tz":-420,"elapsed":12853,"user":{"displayName":"A.M Dirham Dewantara","photoUrl":"https://lh5.googleusercontent.com/-WTvjmG5tB8E/AAAAAAAAAAI/AAAAAAAAAPs/wjM4C_zLOm8/s64/photo.jpg","userId":"02029333175577307706"}},"colab":{"base_uri":"https://localhost:8080/","height":36}},"source":["targets = tf.placeholder(tf.int32, [batch_size, seq_length]) # a 60x50\n","targets"],"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor 'Placeholder_1:0' shape=(60, 50) dtype=int32>"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"markdown","metadata":{"id":"guUXXmU6bNvA","colab_type":"text"},"source":["The memory state of the network is initialized with a vector of zeros and gets updated after reading each character.\n","\n","__BasicRNNCell.zero_state(batch_size, dtype)__ Return zero-filled state tensor(s). In this function, batch_size\n","representing the batch size."]},{"cell_type":"code","metadata":{"id":"tZSR7NB-bNvB","colab_type":"code","colab":{}},"source":["initial_state = stacked_cell.zero_state(batch_size, tf.float32) #why batch_size ? 60x128"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"2fcHIefebNvG","colab_type":"text"},"source":["Lets check the value of the input_data again:"]},{"cell_type":"code","metadata":{"id":"oh59vOWMbNvJ","colab_type":"code","outputId":"be59a63b-e538-4f00-b7b8-120aa588f353","executionInfo":{"status":"ok","timestamp":1565462027194,"user_tz":-420,"elapsed":14396,"user":{"displayName":"A.M Dirham Dewantara","photoUrl":"https://lh5.googleusercontent.com/-WTvjmG5tB8E/AAAAAAAAAAI/AAAAAAAAAPs/wjM4C_zLOm8/s64/photo.jpg","userId":"02029333175577307706"}},"colab":{"base_uri":"https://localhost:8080/","height":148}},"source":["session = tf.Session()\n","feed_dict={input_data:x, targets:y}\n","session.run(input_data, feed_dict)"],"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[64, 10,  0, ..., 10,  3,  4],\n","       [23,  1, 12, ...,  7,  6,  1],\n","       [ 0,  2,  1, ...,  0, 14,  0],\n","       ...,\n","       [ 1, 10,  3, ...,  2,  1,  5],\n","       [ 3, 12,  0, ...,  5,  2, 14],\n","       [ 7,  5,  1, ...,  4, 12,  0]], dtype=int32)"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"markdown","metadata":{"id":"kcLkGGUSbNvO","colab_type":"text"},"source":["### Embedding\n","In this section, we build a 128-dim vector for each character. As we have 60 batches, and 50 character in each sequence, it will generate a [60,50,128] matrix."]},{"cell_type":"markdown","metadata":{"id":"OBcn2cQ4bNvP","colab_type":"text"},"source":["__Notice:__ The function `tf.get_variable()` is used to share a variable and to initialize it in one place. `tf.get_variable()` is used to get or create a variable instead of a direct call to `tf.Variable`. "]},{"cell_type":"code","metadata":{"id":"kDAZCHFcbNvQ","colab_type":"code","outputId":"fac69a09-e70e-43a2-a18c-501eca42642e","executionInfo":{"status":"ok","timestamp":1565462027200,"user_tz":-420,"elapsed":14384,"user":{"displayName":"A.M Dirham Dewantara","photoUrl":"https://lh5.googleusercontent.com/-WTvjmG5tB8E/AAAAAAAAAAI/AAAAAAAAAPs/wjM4C_zLOm8/s64/photo.jpg","userId":"02029333175577307706"}},"colab":{"base_uri":"https://localhost:8080/","height":93}},"source":["with tf.variable_scope('rnnlm', reuse=False):\n","    softmax_w = tf.get_variable(\"softmax_w\", [rnn_size, vocab_size]) #128x65\n","    softmax_b = tf.get_variable(\"softmax_b\", [vocab_size]) # 1x65)\n","    #with tf.device(\"/cpu:0\"):\n","        \n","    # embedding variable is initialized randomely\n","    embedding = tf.get_variable(\"embedding\", [vocab_size, rnn_size])  #65x128\n","\n","    # embedding_lookup goes to each row of input_data, and for each character in the row, finds the correspond vector in embedding\n","    # it creates a 60*50*[1*128] matrix\n","    # so, the first elemnt of em, is a matrix of 50x128, which each row of it is vector representing that character\n","    em = tf.nn.embedding_lookup(embedding, input_data) # em is 60x50x[1*128]\n","    # split: Splits a tensor into sub tensors.\n","    # syntax:  tf.split(split_dim, num_split, value, name='split')\n","    # it will split the 60x50x[1x128] matrix into 50 matrix of 60x[1*128]\n","    inputs = tf.split(em, seq_length, 1)\n","    # It will convert the list to 50 matrix of [60x128]\n","    inputs = [tf.squeeze(input_, [1]) for input_ in inputs]"],"execution_count":18,"outputs":[{"output_type":"stream","text":["W0810 18:33:36.100166 139916882519936 deprecation.py:506] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling __init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"4mC8M0IobNvT","colab_type":"text"},"source":["Lets take a look at the __embedding__, __em__, and __inputs__ variabbles:\n","\n","Embedding variable is initialized with random values:"]},{"cell_type":"code","metadata":{"id":"OhKRvPNQbNvT","colab_type":"code","outputId":"bd6909cf-4a94-47a2-db54-3b756b59397a","executionInfo":{"status":"ok","timestamp":1565462029495,"user_tz":-420,"elapsed":16661,"user":{"displayName":"A.M Dirham Dewantara","photoUrl":"https://lh5.googleusercontent.com/-WTvjmG5tB8E/AAAAAAAAAAI/AAAAAAAAAPs/wjM4C_zLOm8/s64/photo.jpg","userId":"02029333175577307706"}},"colab":{"base_uri":"https://localhost:8080/","height":261}},"source":["session.run(tf.global_variables_initializer())\n","#print embedding.shape\n","session.run(embedding)"],"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[-0.02841072, -0.01901975,  0.01223855, ...,  0.1535712 ,\n","         0.16273122,  0.05578588],\n","       [ 0.10786624, -0.1490694 , -0.16838174, ..., -0.00149123,\n","         0.07777567,  0.02358611],\n","       [-0.12915027,  0.06553157, -0.1686174 , ...,  0.10710572,\n","         0.17577808,  0.01123339],\n","       ...,\n","       [-0.14464302,  0.14764939,  0.10011251, ...,  0.11694588,\n","         0.1092384 ,  0.03756763],\n","       [-0.16558893,  0.14222057, -0.13517344, ..., -0.13581048,\n","         0.06933089,  0.03147173],\n","       [ 0.12193535, -0.02072264,  0.16026728, ..., -0.12554412,\n","         0.03571533, -0.12724882]], dtype=float32)"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"markdown","metadata":{"id":"AdQutxeRbNvi","colab_type":"text"},"source":["The first elemnt of em, is a matrix of 50x128, which each row of it is vector representing that character"]},{"cell_type":"code","metadata":{"id":"GB_hgyUnbNvj","colab_type":"code","outputId":"ed9aa226-74d8-498f-9119-6fb568fe9543","executionInfo":{"status":"ok","timestamp":1565462029501,"user_tz":-420,"elapsed":16651,"user":{"displayName":"A.M Dirham Dewantara","photoUrl":"https://lh5.googleusercontent.com/-WTvjmG5tB8E/AAAAAAAAAAI/AAAAAAAAAPs/wjM4C_zLOm8/s64/photo.jpg","userId":"02029333175577307706"}},"colab":{"base_uri":"https://localhost:8080/","height":279}},"source":["em = tf.nn.embedding_lookup(embedding, input_data)\n","emp = session.run(em,feed_dict={input_data:x})\n","print emp.shape\n","emp[0]"],"execution_count":20,"outputs":[{"output_type":"stream","text":["(60, 50, 128)\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["array([[ 0.12193535, -0.02072264,  0.16026728, ..., -0.12554412,\n","         0.03571533, -0.12724882],\n","       [ 0.05441378, -0.13205174, -0.1530728 , ...,  0.14034592,\n","         0.09010203, -0.14989766],\n","       [-0.02841072, -0.01901975,  0.01223855, ...,  0.1535712 ,\n","         0.16273122,  0.05578588],\n","       ...,\n","       [ 0.05441378, -0.13205174, -0.1530728 , ...,  0.14034592,\n","         0.09010203, -0.14989766],\n","       [ 0.15039589,  0.07008369, -0.02662542, ...,  0.1645946 ,\n","        -0.0713301 , -0.14769801],\n","       [ 0.10482858,  0.034015  ,  0.0174195 , ...,  0.06627652,\n","         0.04537097,  0.10749434]], dtype=float32)"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"markdown","metadata":{"id":"lslWRjGSbNvn","colab_type":"text"},"source":["Let's consider each sequence as a sentence of length 50 characters, then, the first item in __inputs__ is a [60x128] vector which represents the first characters of 60 sentences."]},{"cell_type":"code","metadata":{"id":"wsDXNh67bNvo","colab_type":"code","outputId":"f59aa1a0-1622-48a0-b2f6-0fd8c2d1a842","executionInfo":{"status":"ok","timestamp":1565462029505,"user_tz":-420,"elapsed":16637,"user":{"displayName":"A.M Dirham Dewantara","photoUrl":"https://lh5.googleusercontent.com/-WTvjmG5tB8E/AAAAAAAAAAI/AAAAAAAAAPs/wjM4C_zLOm8/s64/photo.jpg","userId":"02029333175577307706"}},"colab":{"base_uri":"https://localhost:8080/","height":111}},"source":["inputs = tf.split(em, seq_length, 1)\n","inputs = [tf.squeeze(input_, [1]) for input_ in inputs]\n","inputs[0:5]"],"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[<tf.Tensor 'Squeeze:0' shape=(60, 128) dtype=float32>,\n"," <tf.Tensor 'Squeeze_1:0' shape=(60, 128) dtype=float32>,\n"," <tf.Tensor 'Squeeze_2:0' shape=(60, 128) dtype=float32>,\n"," <tf.Tensor 'Squeeze_3:0' shape=(60, 128) dtype=float32>,\n"," <tf.Tensor 'Squeeze_4:0' shape=(60, 128) dtype=float32>]"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"markdown","metadata":{"id":"QBPV3tLubNvq","colab_type":"text"},"source":["### Feeding a batch of 50 sequence to a RNN:\n","\n","The feeding process for iputs is as following:\n","\n","- Step 1:  first character of each of the 50 sentences (in a batch) is entered in parallel.  \n","- Step 2:  second character of each of the 50 sentences is input in parallel. \n","- Step n: nth character of each of the 50 sentences is input in parallel.  \n","\n","The parallelism is only for efficiency.  Each character in a batch is handled in parallel,  but the network sees one character of a sequence at a time and does the computations accordingly. All the computations involving the characters of all sequences in a batch at a given time step are done in parallel. "]},{"cell_type":"code","metadata":{"id":"zSUJElNobNvq","colab_type":"code","outputId":"d7812db0-9f58-40f1-a5d9-7e1d191f6d31","executionInfo":{"status":"ok","timestamp":1565462029507,"user_tz":-420,"elapsed":16621,"user":{"displayName":"A.M Dirham Dewantara","photoUrl":"https://lh5.googleusercontent.com/-WTvjmG5tB8E/AAAAAAAAAAI/AAAAAAAAAPs/wjM4C_zLOm8/s64/photo.jpg","userId":"02029333175577307706"}},"colab":{"base_uri":"https://localhost:8080/","height":261}},"source":["session.run(inputs[0],feed_dict={input_data:x})"],"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[ 0.12193535, -0.02072264,  0.16026728, ..., -0.12554412,\n","         0.03571533, -0.12724882],\n","       [-0.1129979 ,  0.16265441,  0.09154923, ..., -0.01499201,\n","        -0.01257834,  0.01521023],\n","       [-0.02841072, -0.01901975,  0.01223855, ...,  0.1535712 ,\n","         0.16273122,  0.05578588],\n","       ...,\n","       [ 0.10786624, -0.1490694 , -0.16838174, ..., -0.00149123,\n","         0.07777567,  0.02358611],\n","       [ 0.15039589,  0.07008369, -0.02662542, ...,  0.1645946 ,\n","        -0.0713301 , -0.14769801],\n","       [-0.05472646, -0.08993981,  0.15164237, ..., -0.14542383,\n","        -0.02694488, -0.06578686]], dtype=float32)"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"markdown","metadata":{"id":"Jperu_KrbNvt","colab_type":"text"},"source":["Feeding the RNN with one batch, we can check the new output and new state of network:"]},{"cell_type":"code","metadata":{"id":"cUmWamqlbNvu","colab_type":"code","outputId":"443ec7a1-bae3-4efb-de79-8932d94fb0b6","executionInfo":{"status":"ok","timestamp":1565462030284,"user_tz":-420,"elapsed":17380,"user":{"displayName":"A.M Dirham Dewantara","photoUrl":"https://lh5.googleusercontent.com/-WTvjmG5tB8E/AAAAAAAAAAI/AAAAAAAAAPs/wjM4C_zLOm8/s64/photo.jpg","userId":"02029333175577307706"}},"colab":{"base_uri":"https://localhost:8080/","height":131}},"source":["#outputs is 50x[60*128]\n","outputs, new_state = tf.contrib.legacy_seq2seq.rnn_decoder(inputs, initial_state, stacked_cell, loop_function=None, scope='rnnlm')\n","new_state"],"execution_count":23,"outputs":[{"output_type":"stream","text":["W0810 18:33:38.686130 139916882519936 deprecation.py:506] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn_cell_impl.py:459: calling __init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["(<tf.Tensor 'rnnlm_1/rnnlm/multi_rnn_cell/cell_0/basic_rnn_cell/Tanh_98:0' shape=(60, 128) dtype=float32>,\n"," <tf.Tensor 'rnnlm_1/rnnlm/multi_rnn_cell/cell_0/basic_rnn_cell/Tanh_99:0' shape=(60, 128) dtype=float32>)"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"code","metadata":{"id":"1YHOqLqbbNvx","colab_type":"code","outputId":"7e3acdc5-faa2-4b6b-d7ca-f933269f9e1e","executionInfo":{"status":"ok","timestamp":1565462030289,"user_tz":-420,"elapsed":17368,"user":{"displayName":"A.M Dirham Dewantara","photoUrl":"https://lh5.googleusercontent.com/-WTvjmG5tB8E/AAAAAAAAAAI/AAAAAAAAAPs/wjM4C_zLOm8/s64/photo.jpg","userId":"02029333175577307706"}},"colab":{"base_uri":"https://localhost:8080/","height":111}},"source":["outputs[0:5]"],"execution_count":24,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[<tf.Tensor 'rnnlm_1/rnnlm/multi_rnn_cell/cell_0/basic_rnn_cell/Tanh_1:0' shape=(60, 128) dtype=float32>,\n"," <tf.Tensor 'rnnlm_1/rnnlm/multi_rnn_cell/cell_0/basic_rnn_cell/Tanh_3:0' shape=(60, 128) dtype=float32>,\n"," <tf.Tensor 'rnnlm_1/rnnlm/multi_rnn_cell/cell_0/basic_rnn_cell/Tanh_5:0' shape=(60, 128) dtype=float32>,\n"," <tf.Tensor 'rnnlm_1/rnnlm/multi_rnn_cell/cell_0/basic_rnn_cell/Tanh_7:0' shape=(60, 128) dtype=float32>,\n"," <tf.Tensor 'rnnlm_1/rnnlm/multi_rnn_cell/cell_0/basic_rnn_cell/Tanh_9:0' shape=(60, 128) dtype=float32>]"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"markdown","metadata":{"id":"d1hujbjXbNv0","colab_type":"text"},"source":["Let's check the output of network after feeding it with first batch:"]},{"cell_type":"code","metadata":{"id":"ttkJiIbNbNv1","colab_type":"code","outputId":"9fb8595a-0411-4328-d1ba-9a9f40e290da","executionInfo":{"status":"ok","timestamp":1565462031562,"user_tz":-420,"elapsed":18625,"user":{"displayName":"A.M Dirham Dewantara","photoUrl":"https://lh5.googleusercontent.com/-WTvjmG5tB8E/AAAAAAAAAAI/AAAAAAAAAPs/wjM4C_zLOm8/s64/photo.jpg","userId":"02029333175577307706"}},"colab":{"base_uri":"https://localhost:8080/","height":261}},"source":["first_output = outputs[0]\n","session.run(tf.global_variables_initializer())\n","session.run(first_output,feed_dict={input_data:x})"],"execution_count":25,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[-0.08565257,  0.00845711, -0.00484119, ...,  0.01466321,\n","        -0.04042432,  0.02968367],\n","       [-0.07553703, -0.10835049,  0.00602222, ..., -0.11093538,\n","        -0.07466232,  0.03257202],\n","       [-0.0069778 , -0.11565599,  0.01203423, ...,  0.04485587,\n","         0.08198409, -0.02173362],\n","       ...,\n","       [ 0.05269023,  0.00162428, -0.07521877, ..., -0.00202964,\n","        -0.00433859, -0.06750776],\n","       [-0.05769448,  0.08039131,  0.0933935 , ...,  0.05215279,\n","         0.06394163, -0.04821733],\n","       [ 0.02370692,  0.08285926,  0.05262995, ..., -0.00427394,\n","         0.05966717,  0.039359  ]], dtype=float32)"]},"metadata":{"tags":[]},"execution_count":25}]},{"cell_type":"markdown","metadata":{"id":"jEkWItLtbNv4","colab_type":"text"},"source":["As it was explained, __outputs__ variable is a 50x[60x128] tensor. We need to reshape it back to [60x50x128] to be able to calculate the probablity of the next character using the softmax. The __softmax_w__ shape is [rnn_size, vocab_size],whihc is [128x65] in our case. Threfore, we have a fully connected layer on top of LSTM cells, which help us to decode the next charachter. We can use the __softmax(output * softmax_w + softmax_b)__ for this purpose. The shape of the matrixis would be:\n","\n","softmax([60x50x128]x[128x65]+[1x65]) = [60x50x65]"]},{"cell_type":"markdown","metadata":{"id":"vvW7JjbibNv5","colab_type":"text"},"source":["We can do it step-by-step:"]},{"cell_type":"code","metadata":{"id":"Yz1NlE-ZbNv7","colab_type":"code","outputId":"a33c44d1-74e4-437c-e1ec-22db0ba44373","executionInfo":{"status":"ok","timestamp":1565462031565,"user_tz":-420,"elapsed":18611,"user":{"displayName":"A.M Dirham Dewantara","photoUrl":"https://lh5.googleusercontent.com/-WTvjmG5tB8E/AAAAAAAAAAI/AAAAAAAAAPs/wjM4C_zLOm8/s64/photo.jpg","userId":"02029333175577307706"}},"colab":{"base_uri":"https://localhost:8080/","height":36}},"source":["output = tf.reshape(tf.concat( outputs,1), [-1, rnn_size])\n","output"],"execution_count":26,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor 'Reshape:0' shape=(3000, 128) dtype=float32>"]},"metadata":{"tags":[]},"execution_count":26}]},{"cell_type":"code","metadata":{"id":"rDZRqOfSbNv_","colab_type":"code","outputId":"340d6c4b-d289-4a07-b565-faa82a00ea72","executionInfo":{"status":"ok","timestamp":1565462031567,"user_tz":-420,"elapsed":18599,"user":{"displayName":"A.M Dirham Dewantara","photoUrl":"https://lh5.googleusercontent.com/-WTvjmG5tB8E/AAAAAAAAAAI/AAAAAAAAAPs/wjM4C_zLOm8/s64/photo.jpg","userId":"02029333175577307706"}},"colab":{"base_uri":"https://localhost:8080/","height":36}},"source":["logits = tf.matmul(output, softmax_w) + softmax_b\n","logits"],"execution_count":27,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor 'add:0' shape=(3000, 65) dtype=float32>"]},"metadata":{"tags":[]},"execution_count":27}]},{"cell_type":"code","metadata":{"id":"n9KRHdLybNwB","colab_type":"code","outputId":"154c9214-719e-410d-a165-2bd6aa0c75be","executionInfo":{"status":"ok","timestamp":1565462031568,"user_tz":-420,"elapsed":18584,"user":{"displayName":"A.M Dirham Dewantara","photoUrl":"https://lh5.googleusercontent.com/-WTvjmG5tB8E/AAAAAAAAAAI/AAAAAAAAAPs/wjM4C_zLOm8/s64/photo.jpg","userId":"02029333175577307706"}},"colab":{"base_uri":"https://localhost:8080/","height":36}},"source":["probs = tf.nn.softmax(logits)\n","probs"],"execution_count":28,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor 'Softmax:0' shape=(3000, 65) dtype=float32>"]},"metadata":{"tags":[]},"execution_count":28}]},{"cell_type":"markdown","metadata":{"id":"wg3IuxhCbNwE","colab_type":"text"},"source":["Here is the probablity of the next chracter in all batches:"]},{"cell_type":"code","metadata":{"id":"2t_ASQYDbNwF","colab_type":"code","outputId":"955e7e67-4112-43c9-a692-1255f86fae05","executionInfo":{"status":"ok","timestamp":1565462031570,"user_tz":-420,"elapsed":18570,"user":{"displayName":"A.M Dirham Dewantara","photoUrl":"https://lh5.googleusercontent.com/-WTvjmG5tB8E/AAAAAAAAAAI/AAAAAAAAAPs/wjM4C_zLOm8/s64/photo.jpg","userId":"02029333175577307706"}},"colab":{"base_uri":"https://localhost:8080/","height":261}},"source":["session.run(tf.global_variables_initializer())\n","session.run(probs,feed_dict={input_data:x})"],"execution_count":29,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0.01309517, 0.01667932, 0.01198887, ..., 0.01367628, 0.01105585,\n","        0.01197051],\n","       [0.01361866, 0.01560324, 0.0122492 , ..., 0.01466775, 0.01302625,\n","        0.01302935],\n","       [0.01401642, 0.01460189, 0.01385708, ..., 0.01150042, 0.01239447,\n","        0.01603002],\n","       ...,\n","       [0.01413262, 0.0142019 , 0.01383806, ..., 0.01332967, 0.01503977,\n","        0.01989389],\n","       [0.01687989, 0.0140704 , 0.01310409, ..., 0.01397498, 0.01497899,\n","        0.01482976],\n","       [0.01435259, 0.01432823, 0.00864147, ..., 0.0211857 , 0.01382184,\n","        0.01570775]], dtype=float32)"]},"metadata":{"tags":[]},"execution_count":29}]},{"cell_type":"markdown","metadata":{"id":"_OofODRZbNwK","colab_type":"text"},"source":["Now, we are in the position to calculate the cost of training with __loss function__, and keep feedng the network to learn it. But, the question is: what the LSTM networks learn?"]},{"cell_type":"code","metadata":{"id":"WGILTShubNwK","colab_type":"code","outputId":"7a037877-3019-4b3f-98d5-5b404bef1ad3","executionInfo":{"status":"ok","timestamp":1565462031574,"user_tz":-420,"elapsed":18558,"user":{"displayName":"A.M Dirham Dewantara","photoUrl":"https://lh5.googleusercontent.com/-WTvjmG5tB8E/AAAAAAAAAAI/AAAAAAAAAPs/wjM4C_zLOm8/s64/photo.jpg","userId":"02029333175577307706"}},"colab":{"base_uri":"https://localhost:8080/","height":111}},"source":["grad_clip =5.\n","tvars = tf.trainable_variables()\n","tvars"],"execution_count":30,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[<tf.Variable 'rnnlm/softmax_w:0' shape=(128, 65) dtype=float32_ref>,\n"," <tf.Variable 'rnnlm/softmax_b:0' shape=(65,) dtype=float32_ref>,\n"," <tf.Variable 'rnnlm/embedding:0' shape=(65, 128) dtype=float32_ref>,\n"," <tf.Variable 'rnnlm/multi_rnn_cell/cell_0/basic_rnn_cell/kernel:0' shape=(256, 128) dtype=float32_ref>,\n"," <tf.Variable 'rnnlm/multi_rnn_cell/cell_0/basic_rnn_cell/bias:0' shape=(128,) dtype=float32_ref>]"]},"metadata":{"tags":[]},"execution_count":30}]},{"cell_type":"markdown","metadata":{"id":"Ry7l5xWzbNwM","colab_type":"text"},"source":["# All together\n","Now, let's put all of parts together in a class, and train the model:"]},{"cell_type":"code","metadata":{"id":"zs9_kix4bNwM","colab_type":"code","colab":{}},"source":["class LSTMModel():\n","    def __init__(self,sample=False):\n","        rnn_size = 128 # size of RNN hidden state vector\n","        batch_size = 60 # minibatch size, i.e. size of dataset in each epoch\n","        seq_length = 50 # RNN sequence length\n","        num_layers = 2 # number of layers in the RNN\n","        vocab_size = 65\n","        grad_clip = 5.\n","        if sample:\n","            print(\">> sample mode:\")\n","            batch_size = 1\n","            seq_length = 1\n","        # The core of the model consists of an LSTM cell that processes one char at a time and computes probabilities of the possible continuations of the char. \n","        basic_cell = tf.contrib.rnn.BasicRNNCell(rnn_size)\n","        # model.cell.state_size is (128, 128)\n","        self.stacked_cell = tf.contrib.rnn.MultiRNNCell([basic_cell] * num_layers)\n","\n","        self.input_data = tf.placeholder(tf.int32, [batch_size, seq_length], name=\"input_data\")\n","        self.targets = tf.placeholder(tf.int32, [batch_size, seq_length], name=\"targets\")\n","        # Initial state of the LSTM memory.\n","        # The memory state of the network is initialized with a vector of zeros and gets updated after reading each char. \n","        self.initial_state = stacked_cell.zero_state(batch_size, tf.float32) #why batch_size\n","\n","        with tf.variable_scope('rnnlm_class1'):\n","            softmax_w = tf.get_variable(\"softmax_w\", [rnn_size, vocab_size]) #128x65\n","            softmax_b = tf.get_variable(\"softmax_b\", [vocab_size]) # 1x65\n","            with tf.device(\"/cpu:0\"):\n","                embedding = tf.get_variable(\"embedding\", [vocab_size, rnn_size])  #65x128\n","                inputs = tf.split(tf.nn.embedding_lookup(embedding, self.input_data), seq_length, 1)\n","                inputs = [tf.squeeze(input_, [1]) for input_ in inputs]\n","                #inputs = tf.split(em, seq_length, 1)\n","                \n","                \n","\n","\n","        # The value of state is updated after processing each batch of chars.\n","        outputs, last_state = tf.contrib.legacy_seq2seq.rnn_decoder(inputs, self.initial_state, self.stacked_cell, loop_function=None, scope='rnnlm_class1')\n","        output = tf.reshape(tf.concat(outputs,1), [-1, rnn_size])\n","        self.logits = tf.matmul(output, softmax_w) + softmax_b\n","        self.probs = tf.nn.softmax(self.logits)\n","        loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example([self.logits],\n","                [tf.reshape(self.targets, [-1])],\n","                [tf.ones([batch_size * seq_length])],\n","                vocab_size)\n","        self.cost = tf.reduce_sum(loss) / batch_size / seq_length\n","        self.final_state = last_state\n","        self.lr = tf.Variable(0.0, trainable=False)\n","        tvars = tf.trainable_variables()\n","        grads, _ = tf.clip_by_global_norm(tf.gradients(self.cost, tvars),grad_clip)\n","        optimizer = tf.train.AdamOptimizer(self.lr)\n","        self.train_op = optimizer.apply_gradients(zip(grads, tvars))\n","    \n","    \n","    def sample(self, sess, chars, vocab, num=200, prime='adalah ', sampling_type=1):\n","        state = sess.run(self.stacked_cell.zero_state(1, tf.float32))\n","        #print state\n","        for char in prime[:-1]:\n","            x = np.zeros((1, 1))\n","            x[0, 0] = vocab[char]\n","            feed = {self.input_data: x, self.initial_state:state}\n","            [state] = sess.run([self.final_state], feed)\n","\n","        def weighted_pick(weights):\n","            t = np.cumsum(weights)\n","            s = np.sum(weights)\n","            return(int(np.searchsorted(t, np.random.rand(1)*s)))\n","\n","        ret = prime\n","        char = prime[-1]\n","        for n in range(num):\n","            x = np.zeros((1, 1))\n","            x[0, 0] = vocab[char]\n","            feed = {self.input_data: x, self.initial_state:state}\n","            [probs, state] = sess.run([self.probs, self.final_state], feed)\n","            p = probs[0]\n","\n","            if sampling_type == 0:\n","                sample = np.argmax(p)\n","            elif sampling_type == 2:\n","                if char == ' ':\n","                    sample = weighted_pick(p)\n","                else:\n","                    sample = np.argmax(p)\n","            else: # sampling_type == 1 default:\n","                sample = weighted_pick(p)\n","\n","            pred = chars[sample]\n","            ret += pred\n","            char = pred\n","        return ret"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"ZKzlRmjvbNwP","colab_type":"text"},"source":["### Creating the LSTM object\n","Now we create a LSTM model:"]},{"cell_type":"code","metadata":{"id":"NFOoC70ZbNwQ","colab_type":"code","outputId":"adb0c478-0418-4888-b5df-e09bb9f7b187","executionInfo":{"status":"ok","timestamp":1565462033637,"user_tz":-420,"elapsed":20593,"user":{"displayName":"A.M Dirham Dewantara","photoUrl":"https://lh5.googleusercontent.com/-WTvjmG5tB8E/AAAAAAAAAAI/AAAAAAAAAPs/wjM4C_zLOm8/s64/photo.jpg","userId":"02029333175577307706"}},"colab":{"base_uri":"https://localhost:8080/","height":93}},"source":["with tf.variable_scope(\"rnn\"):\n","    model = LSTMModel()"],"execution_count":32,"outputs":[{"output_type":"stream","text":["W0810 18:33:42.326910 139916882519936 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/clip_ops.py:286: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"oEdW5t7qbNwT","colab_type":"text"},"source":["# Train usinng LSTMModel class\n","We can train our model through feeding batches:"]},{"cell_type":"code","metadata":{"id":"3fHHML6sbNwV","colab_type":"code","outputId":"6531194c-30bd-4519-ceb8-d42ddfaf132c","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1565462297992,"user_tz":-420,"elapsed":284930,"user":{"displayName":"A.M Dirham Dewantara","photoUrl":"https://lh5.googleusercontent.com/-WTvjmG5tB8E/AAAAAAAAAAI/AAAAAAAAAPs/wjM4C_zLOm8/s64/photo.jpg","userId":"02029333175577307706"}}},"source":["with tf.Session() as sess:\n","    sess.run(tf.global_variables_initializer())\n","    for e in range(num_epochs): # num_epochs is 5 for test, but should be higher\n","        sess.run(tf.assign(model.lr, learning_rate * (decay_rate ** e)))\n","        data_loader.reset_batch_pointer()\n","        state = sess.run(model.initial_state) # (2x[60x128])\n","        for b in range(data_loader.num_batches): #for each batch\n","            start = time.time()\n","            x, y = data_loader.next_batch()\n","            feed = {model.input_data: x, model.targets: y, model.initial_state:state}\n","            train_loss, state, _ = sess.run([model.cost, model.final_state, model.train_op], feed)\n","            end = time.time()\n","        print(\"{}/{} (epoch {}), train_loss = {:.3f}, time/batch = {:.3f}\" \\\n","                .format(e * data_loader.num_batches + b, num_epochs * data_loader.num_batches, e, train_loss, end - start))\n","        with tf.variable_scope(\"rnn\", reuse=True):\n","            sample_model = LSTMModel(sample=True)\n","            print sample_model.sample(sess, data_loader.chars , data_loader.vocab, num=50, prime='Saya ', sampling_type=1)\n","            print '----------------------------------'"],"execution_count":33,"outputs":[{"output_type":"stream","text":["1/250 (epoch 0), train_loss = 4.027, time/batch = 0.019\n",">> sample mode:\n","Saya BeweMdH)9mjmy IAn﻿pTBA gC 3 TC)\n",")f(g1.z3Eyutb/vmv3\n","----------------------------------\n","3/250 (epoch 1), train_loss = 3.525, time/batch = 0.023\n",">> sample mode:\n","Saya enoTksi9ikk9k!Wkn﻿-Phsvapeke1owDL3kAsGykrs  oyTUoP\n","----------------------------------\n","5/250 (epoch 2), train_loss = 3.219, time/batch = 0.018\n",">> sample mode:\n","Saya 95kayT(jtWtao gylak AJ   me    baSAinis jran-iwryT\n","----------------------------------\n","7/250 (epoch 3), train_loss = 3.075, time/batch = 0.018\n",">> sample mode:\n","Saya homEbaluipa n Man a tkneage oowikeoir,rmWP nay 3k \n","----------------------------------\n","9/250 (epoch 4), train_loss = 3.017, time/batch = 0.017\n",">> sample mode:\n","Saya m gsgnnaa.ijaananan)nsagtnnnneolaoane maag nWp jnn\n","----------------------------------\n","11/250 (epoch 5), train_loss = 2.955, time/batch = 0.017\n",">> sample mode:\n","Saya atega ik yayn5ae  ldimea,ekaagaroejlsiuel ilaaiauk\n","----------------------------------\n","13/250 (epoch 6), train_loss = 2.898, time/batch = 0.017\n",">> sample mode:\n","Saya goPaaWauucui ciubg  a,seugtfumnsTePdkya\n","gdt iil et\n","----------------------------------\n","15/250 (epoch 7), train_loss = 2.837, time/batch = 0.017\n",">> sample mode:\n","Saya akKthaNen. dmrims ie)nnt  i. sini rcnaBm o,a-re in\n","----------------------------------\n","17/250 (epoch 8), train_loss = 2.773, time/batch = 0.020\n",">> sample mode:\n","Saya  bdAu sewd lHpal/(garaia .itArgtr aeisnh il,.leKes\n","----------------------------------\n","19/250 (epoch 9), train_loss = 2.716, time/batch = 0.017\n",">> sample mode:\n","Saya bKiee eesJa Jrop nenbsl raribeEpi mEta,  ala  rkp)\n","----------------------------------\n","21/250 (epoch 10), train_loss = 2.668, time/batch = 0.017\n",">> sample mode:\n","Saya  omia(a Ta yekachfk..pn bndMa  anzhKe,gaderenspaa \n","----------------------------------\n","23/250 (epoch 11), train_loss = 2.624, time/batch = 0.022\n",">> sample mode:\n","Saya yiwal lgnstsjtciFk.na aansatkKasnsaegtnakJegKSaB l\n","----------------------------------\n","25/250 (epoch 12), train_loss = 2.579, time/batch = 0.019\n",">> sample mode:\n","Saya aeuasnkiku srnror nau jebasa  yleailaa  m\n","ta(seury\n","----------------------------------\n","27/250 (epoch 13), train_loss = 2.535, time/batch = 0.021\n",">> sample mode:\n","Saya aiakalitgByHk aikik Poni mibail neosa pdnaBIat kan\n","----------------------------------\n","29/250 (epoch 14), train_loss = 2.496, time/batch = 0.018\n",">> sample mode:\n","Saya s-rlanUi-rcyKkyt,9,k u nknhalsJ PaiaMe)syBjnzhstun\n","----------------------------------\n","31/250 (epoch 15), train_loss = 2.461, time/batch = 0.018\n",">> sample mode:\n","Saya PPrboesao lamiwuytsnPsr laliPa aetg tamauh yemdan \n","----------------------------------\n","33/250 (epoch 16), train_loss = 2.430, time/batch = 0.017\n",">> sample mode:\n","Saya yenytekan uuu kelasbanguK Pdabe,oca ujt loka sutek\n","----------------------------------\n","35/250 (epoch 17), train_loss = 2.399, time/batch = 0.017\n",">> sample mode:\n","Saya b\n","bcxaag janG reka gang memjopartan moya cemau tem\n","----------------------------------\n","37/250 (epoch 18), train_loss = 2.370, time/batch = 0.017\n",">> sample mode:\n","Saya aeslkak  oo.a giyap teboa mebe tegany tenoelayg Me\n","----------------------------------\n","39/250 (epoch 19), train_loss = 2.343, time/batch = 0.017\n",">> sample mode:\n","Saya melim seldel denuelaka aeni raryaddWiB,luku, mekcz\n","----------------------------------\n","41/250 (epoch 20), train_loss = 2.317, time/batch = 0.017\n",">> sample mode:\n","Saya )eca dannTta .elakaka nay kbli rajaHyang semah Dah\n","----------------------------------\n","43/250 (epoch 21), train_loss = 2.292, time/batch = 0.017\n",">> sample mode:\n","Saya Kiltn JtngHnmbamaya tamuk neikak dembisu bung berd\n","----------------------------------\n","45/250 (epoch 22), train_loss = 2.268, time/batch = 0.018\n",">> sample mode:\n","Saya ced Pemana Dayal jpla Poan maay kekim, weaika aene\n","----------------------------------\n","47/250 (epoch 23), train_loss = 2.247, time/batch = 0.017\n",">> sample mode:\n","engajaca.a ailaryeBMalasEemimatukg ,u ao\n","----------------------------------\n","49/250 (epoch 24), train_loss = 2.226, time/batch = 0.017\n",">> sample mode:\n","Saya te.asa san iaan medikibJhat kara nmia feyaH kog t.\n","----------------------------------\n","51/250 (epoch 25), train_loss = 2.206, time/batch = 0.018\n",">> sample mode:\n","Saya semubTtasisma yendalan Paya sedorh danAa timaasan \n","----------------------------------\n","53/250 (epoch 26), train_loss = 2.188, time/batch = 0.017\n",">> sample mode:\n","Saya rauca wiopakang au dan juro basa Selg meoi Jesak b\n","----------------------------------\n","55/250 (epoch 27), train_loss = 2.170, time/batch = 0.017\n",">> sample mode:\n","Saya usi.- eon. tengamali.k sdbkong oinikann BSmaho ys \n","----------------------------------\n","57/250 (epoch 28), train_loss = 2.153, time/batch = 0.018\n",">> sample mode:\n","Saya 9enobang pemaka aakikun sojal -ilatenlaa sab rangn\n","----------------------------------\n","59/250 (epoch 29), train_loss = 2.137, time/batch = 0.023\n",">> sample mode:\n","Saya Ctoos semetamh3\n"," Yebatan Meka sag sainany saMa jam\n","----------------------------------\n","61/250 (epoch 30), train_loss = 2.122, time/batch = 0.017\n",">> sample mode:\n"," Dekin Renrrat, venpesak depa\n","----------------------------------\n","63/250 (epoch 31), train_loss = 2.107, time/batch = 0.018\n",">> sample mode:\n","Saya sarisu aljig Rurn merita tuntikan Sitan ikuban eda\n","----------------------------------\n","65/250 (epoch 32), train_loss = 2.093, time/batch = 0.018\n",">> sample mode:\n","Saya Bdapa Keuga dangh upa leeeelho Gerih. kMkon mra. d\n","----------------------------------\n","67/250 (epoch 33), train_loss = 2.080, time/batch = 0.017\n",">> sample mode:\n"," mongHbata aenumaSK\n","----------------------------------\n","69/250 (epoch 34), train_loss = 2.067, time/batch = 0.018\n",">> sample mode:\n","unit remban spuanhh zatakr kenb caya Adihan \n","----------------------------------\n","71/250 (epoch 35), train_loss = 2.055, time/batch = 0.017\n",">> sample mode:\n","Saya lamikis sana idoyang Ranng upakasak Kowil sadi Ben\n","----------------------------------\n","73/250 (epoch 36), train_loss = 2.043, time/batch = 0.017\n",">> sample mode:\n","Saya Mbotengimaya Kilarg suratuun uluSu kit tadamac ala\n","----------------------------------\n","75/250 (epoch 37), train_loss = 2.032, time/batch = 0.017\n",">> sample mode:\n","Saya denmemieya botugn hataca neya ii, boba meng-tari \n","\n","----------------------------------\n","77/250 (epoch 38), train_loss = 2.021, time/batch = 0.018\n",">> sample mode:\n","Saya )ekulaa Pang -outin seusacad dik Dang caruk .aas t\n","----------------------------------\n","79/250 (epoch 39), train_loss = 2.010, time/batch = 0.017\n",">> sample mode:\n","Saya ndulaKn, s-besa mang berpitingan be-iita suiknyakW\n","----------------------------------\n","81/250 (epoch 40), train_loss = 2.000, time/batch = 0.018\n",">> sample mode:\n","Saya Sasa :ang deliin aeolah san sepbakayanyal Bunyat P\n","----------------------------------\n","83/250 (epoch 41), train_loss = 1.991, time/batch = 0.018\n",">> sample mode:\n","Saya Iho t\n"," sonu banak anya kari dariclan Pe\n","----------------------------------\n","85/250 (epoch 42), train_loss = 1.981, time/batch = 0.019\n",">> sample mode:\n","Saya (darh lemis mo boili(m.npuwan peMilahh kilarjabag \n","----------------------------------\n","87/250 (epoch 43), train_loss = 1.972, time/batch = 0.017\n",">> sample mode:\n","Saya ya-tarin danaya kerilaya soyan tutun .antalJ wa.y \n","----------------------------------\n","89/250 (epoch 44), train_loss = 1.964, time/batch = 0.017\n",">> sample mode:\n","eroelapa\n","----------------------------------\n","91/250 (epoch 45), train_loss = 1.956, time/batch = 0.017\n",">> sample mode:\n","Saya gala SilTannt panya ,ati pemberakga sengaa natar b\n","----------------------------------\n","93/250 (epoch 46), train_loss = 1.948, time/batch = 0.018\n",">> sample mode:\n","Saya ,yatn, delah jang bebandak ybngachik kimakak kan k\n","----------------------------------\n","95/250 (epoch 47), train_loss = 1.940, time/batch = 0.017\n",">> sample mode:\n","Saya fWu1l Kpha Sediki ditengila ati bapa denualar Pbah\n","----------------------------------\n","97/250 (epoch 48), train_loss = 1.932, time/batch = 0.018\n",">> sample mode:\n","Saya yang ihaklajan dalik KemBsyaYo hejbala-ndita(ngah \n","----------------------------------\n","99/250 (epoch 49), train_loss = 1.925, time/batch = 0.017\n",">> sample mode:\n","Saya xeke:i-utapan Delap Lada ding tan. kara lemeibakti\n","----------------------------------\n","101/250 (epoch 50), train_loss = 1.918, time/batch = 0.024\n",">> sample mode:\n","Saya Mbna) kurnmgaya dukgu beresah ratikaya Jaka-tu(dal\n","----------------------------------\n","103/250 (epoch 51), train_loss = 1.912, time/batch = 0.021\n",">> sample mode:\n","Saya yenganay3 i,nKat g yeaS ni\n","-henya larsat samusu me\n","----------------------------------\n","105/250 (epoch 52), train_loss = 1.905, time/batch = 0.017\n",">> sample mode:\n","Saya jbeodikas Ramanya(k. cung Sar. nale.nya memhacsg i\n","----------------------------------\n","107/250 (epoch 53), train_loss = 1.899, time/batch = 0.017\n",">> sample mode:\n","Saya satawa Kbod Binglun ca. Mooika diahk sawa nemImbac\n","----------------------------------\n","109/250 (epoch 54), train_loss = 1.893, time/batch = 0.017\n",">> sample mode:\n","ah beru.aPaya yada mengadan ya-t\n","sa shujan \n","----------------------------------\n","111/250 (epoch 55), train_loss = 1.887, time/batch = 0.017\n",">> sample mode:\n","Saya temulata meng heritaran seya narakit miragan Koya \n","----------------------------------\n","113/250 (epoch 56), train_loss = 1.881, time/batch = 0.020\n",">> sample mode:\n","Saya yata samu)nkeming lelkerPu ?ing engapas nmbabakah \n","----------------------------------\n","115/250 (epoch 57), train_loss = 1.876, time/batch = 0.019\n",">> sample mode:\n","Saya Lemomeklegkaai saya dang mun meraan aha-t deo 1 de\n","----------------------------------\n","117/250 (epoch 58), train_loss = 1.870, time/batch = 0.017\n",">> sample mode:\n","Saya terapa Sallenya,  pubarang pakjeyalapanj Sarat hes\n","----------------------------------\n","119/250 (epoch 59), train_loss = 1.865, time/batch = 0.018\n",">> sample mode:\n","Saya pangaki zadah, Puulata, p3nhenya Tacam Pente tani \n","----------------------------------\n","121/250 (epoch 60), train_loss = 1.860, time/batch = 0.017\n",">> sample mode:\n","Saya P psritang laya Barik lan﻿h keruli ti, tenremi mer\n","----------------------------------\n","123/250 (epoch 61), train_loss = 1.856, time/batch = 0.017\n",">> sample mode:\n","Saya Mantila labehak Kenteraya setsan dutakat deriang c\n","----------------------------------\n","125/250 (epoch 62), train_loss = 1.851, time/batch = 0.018\n",">> sample mode:\n","Saya d AAtika laca gerapaya dehin ipg sSukar,n Botokbca\n","----------------------------------\n","127/250 (epoch 63), train_loss = 1.846, time/batch = 0.019\n",">> sample mode:\n","Saya Hgan ganetarasaya tapiidalah Lelgi fanya tomik ian\n","----------------------------------\n","129/250 (epoch 64), train_loss = 1.842, time/batch = 0.017\n",">> sample mode:\n","Saya HangK tanti serata tiru Wmopik serinvuy. Senida na\n","----------------------------------\n","131/250 (epoch 65), train_loss = 1.838, time/batch = 0.018\n",">> sample mode:\n","Saya baya Kpung beraru temnadat unyan deras jaran Cuki \n","----------------------------------\n","133/250 (epoch 66), train_loss = 1.834, time/batch = 0.017\n",">> sample mode:\n","Saya Hendelaha sah memW\n","oMgan Kela Cer.aEn jemMak Anya \n","----------------------------------\n","135/250 (epoch 67), train_loss = 1.830, time/batch = 0.018\n",">> sample mode:\n","Saya KkibB. rpun tembaya karinya :arlal Kikg Kotal aHy \n","----------------------------------\n","137/250 (epoch 68), train_loss = 1.826, time/batch = 0.018\n",">> sample mode:\n","Saya memingi-kaminga salat. Recwand kI yung mememaKs ls\n","----------------------------------\n","139/250 (epoch 69), train_loss = 1.822, time/batch = 0.017\n",">> sample mode:\n","Saya keCur Kedemi jigar-nate ielat say te:tu ataran, ta\n","----------------------------------\n","141/250 (epoch 70), train_loss = 1.819, time/batch = 0.021\n",">> sample mode:\n","Saya Bajutiya serApudaka  tedii Kanye lanyu daku sias d\n","----------------------------------\n","143/250 (epoch 71), train_loss = 1.815, time/batch = 0.017\n",">> sample mode:\n","Saya CetbeParanBya Panyi, Manusa yang memasadaan tutata\n","----------------------------------\n","145/250 (epoch 72), train_loss = 1.812, time/batch = 0.017\n",">> sample mode:\n","Saya Kho﻿il SUbR-Nakesmla, kayi kanga kocak ianAnPanar \n","----------------------------------\n","147/250 (epoch 73), train_loss = 1.808, time/batch = 0.018\n",">> sample mode:\n","Saya (dedia) serata Bengapau (mekerasa. biwa nedengaa i\n","----------------------------------\n","149/250 (epoch 74), train_loss = 1.805, time/batch = 0.017\n",">> sample mode:\n","Saya kaCi talusi yan reoaanya Kang lemulsaj pengahar ba\n","----------------------------------\n","151/250 (epoch 75), train_loss = 1.802, time/batch = 0.023\n",">> sample mode:\n","Saya ,a:., Sibukn bapa aAnya btu utas mantjanin2 Rapkar\n","----------------------------------\n","153/250 (epoch 76), train_loss = 1.799, time/batch = 0.017\n",">> sample mode:\n","Saya wakak waD an dewisan-uecas ca rongmwknntata semWad\n","----------------------------------\n","155/250 (epoch 77), train_loss = 1.796, time/batch = 0.017\n",">> sample mode:\n","Saya Jengekaa deranntum Khabag  aoya gemenyak saya deng\n","----------------------------------\n","157/250 (epoch 78), train_loss = 1.793, time/batch = 0.017\n",">> sample mode:\n","Saya karinlTatap keroasa mangada sediWPSng61 A3G﻿Ct P, \n","----------------------------------\n","159/250 (epoch 79), train_loss = 1.790, time/batch = 0.017\n",">> sample mode:\n","Saya kata kaya anyang larjak Adulang temuh abala SDpawk\n","----------------------------------\n","161/250 (epoch 80), train_loss = 1.788, time/batch = 0.017\n",">> sample mode:\n","a, genbuI. S-dadi teyang\n","----------------------------------\n","163/250 (epoch 81), train_loss = 1.785, time/batch = 0.017\n",">> sample mode:\n","undikar beramang, berau Maru mulian selia se\n","----------------------------------\n","165/250 (epoch 82), train_loss = 1.782, time/batch = 0.017\n",">> sample mode:\n","Saya yanAh kapi .an dikekikak sJaladan aoi), salagnPha,\n","----------------------------------\n","167/250 (epoch 83), train_loss = 1.780, time/batch = 0.018\n",">> sample mode:\n","Saya BnKok silTa hu, tumut minya aratan bebesanyar seba\n","----------------------------------\n","169/250 (epoch 84), train_loss = 1.778, time/batch = 0.018\n",">> sample mode:\n","ai? seniki o berse\n","----------------------------------\n","171/250 (epoch 85), train_loss = 1.775, time/batch = 0.018\n",">> sample mode:\n","Saya sangan Pu genda yang apa kaman mumalnye teihau Bak\n","----------------------------------\n","173/250 (epoch 86), train_loss = 1.773, time/batch = 0.017\n",">> sample mode:\n","Saya ! dilat.ya janJan ikuda, paru, bs kukewi, seuban d\n","----------------------------------\n","175/250 (epoch 87), train_loss = 1.771, time/batch = 0.022\n",">> sample mode:\n",")ase demang buan litu \n","----------------------------------\n","177/250 (epoch 88), train_loss = 1.769, time/batch = 0.017\n",">> sample mode:\n","Saya K teranga kanat mamin salak ra,i kordake, Pengepar\n","----------------------------------\n","179/250 (epoch 89), train_loss = 1.767, time/batch = 0.017\n",">> sample mode:\n","Saya BuAng yeker mik mengup kentisalan Banu ajija mesel\n","----------------------------------\n","181/250 (epoch 90), train_loss = 1.765, time/batch = 0.017\n",">> sample mode:\n","Saya K bar. HamtayasJ Adawan bewu dang paktaciB Pana se\n","----------------------------------\n","183/250 (epoch 91), train_loss = 1.763, time/batch = 0.017\n",">> sample mode:\n","Saya BeAdakah saya wingan sutuknyan sada, YeAdi Bya Mam\n","----------------------------------\n","185/250 (epoch 92), train_loss = 1.761, time/batch = 0.017\n",">> sample mode:\n","Saya Janya dapai dengac bing lataik berun Wemita bewany\n","----------------------------------\n","187/250 (epoch 93), train_loss = 1.759, time/batch = 0.017\n",">> sample mode:\n","Fjo yang skeru\n","----------------------------------\n","189/250 (epoch 94), train_loss = 1.757, time/batch = 0.017\n",">> sample mode:\n","Saya SongTkar danS dolak dik semikas jitish yiwa Hetit \n","----------------------------------\n","191/250 (epoch 95), train_loss = 1.755, time/batch = 0.018\n",">> sample mode:\n","Saya g p,ikan peitilayan riro ang adera King SorThnwu t\n","----------------------------------\n","193/250 (epoch 96), train_loss = 1.754, time/batch = 0.017\n",">> sample mode:\n","Saya sol/ktuya -eSkak terataka saya Tenaruk sedaata, He\n","----------------------------------\n","195/250 (epoch 97), train_loss = 1.752, time/batch = 0.017\n",">> sample mode:\n","Saya pantarayan Seodalat sinnau .etudahah bebori jaraan\n","----------------------------------\n","197/250 (epoch 98), train_loss = 1.750, time/batch = 0.017\n",">> sample mode:\n","Saya Manturn, Jerajaha selateringjanah dinyating ukdah \n","----------------------------------\n","199/250 (epoch 99), train_loss = 1.749, time/batch = 0.017\n",">> sample mode:\n","Saya tam! beburita sihuk hedan Iuntan pukig sarKan Harl\n","----------------------------------\n","201/250 (epoch 100), train_loss = 1.747, time/batch = 0.017\n",">> sample mode:\n","l\n","\n","a K/m,n sempaharkananY Je\n","----------------------------------\n","203/250 (epoch 101), train_loss = 1.746, time/batch = 0.018\n",">> sample mode:\n","Saya Boutu keran meti serala jpan meRarShar Cbetian lam\n","----------------------------------\n","205/250 (epoch 102), train_loss = 1.745, time/batch = 0.020\n",">> sample mode:\n","Saya mengedkanan kelian man kas saya tedakam kawapi Wad\n","----------------------------------\n","207/250 (epoch 103), train_loss = 1.743, time/batch = 0.017\n",">> sample mode:\n","Saya Bamadan sengana, Mengemisaya seita sila-lim. baina\n","----------------------------------\n","209/250 (epoch 104), train_loss = 1.742, time/batch = 0.017\n",">> sample mode:\n","Saya yaCda Baci jumuk WPdis Keobeh sana ti Mabu, Pindek\n","----------------------------------\n","211/250 (epoch 105), train_loss = 1.741, time/batch = 0.017\n",">> sample mode:\n","Saya danga kopak kukul cecin Kurunn Penduku suya 15,), \n","----------------------------------\n","213/250 (epoch 106), train_loss = 1.739, time/batch = 0.018\n",">> sample mode:\n","Saya Kaya 1, dinaka nenenami masan angwalah  idakaing, \n","----------------------------------\n","215/250 (epoch 107), train_loss = 1.738, time/batch = 0.017\n",">> sample mode:\n","Saya m2merinjut bab, lemikanya, yang dan Wsiin mongan m\n","----------------------------------\n","217/250 (epoch 108), train_loss = 1.737, time/batch = 0.018\n",">> sample mode:\n","Saya Bgaya s!ya litutul Heoi memeada danjWjur-Peng piry\n","----------------------------------\n","219/250 (epoch 109), train_loss = 1.736, time/batch = 0.017\n",">> sample mode:\n"," Asilerity, manyan \n","----------------------------------\n","221/250 (epoch 110), train_loss = 1.735, time/batch = 0.017\n",">> sample mode:\n","Saya Bbara Sihx, Ranar. derap-)ureng kerbu karileteud k\n","----------------------------------\n","223/250 (epoch 111), train_loss = 1.734, time/batch = 0.020\n",">> sample mode:\n","Saya saya, berapai Ga.y,-kembal Hejur beban bao bikau -\n","----------------------------------\n","225/250 (epoch 112), train_loss = 1.732, time/batch = 0.018\n",">> sample mode:\n","Saya sbacu dat kinng beri bou ndaiak Warya Rang beri st\n","----------------------------------\n","227/250 (epoch 113), train_loss = 1.731, time/batch = 0.017\n",">> sample mode:\n","Saya senyenaya dinga tigu pang detbian menpawa dakid ki\n","----------------------------------\n","229/250 (epoch 114), train_loss = 1.730, time/batch = 0.017\n",">> sample mode:\n","Saya sasat (KnAI-yu demitu, Pengiya . kosih ayo tuluh i\n","----------------------------------\n","231/250 (epoch 115), train_loss = 1.729, time/batch = 0.017\n",">> sample mode:\n","Saya Baba) Pii1 Keojam ding.H imatang mumikYac i1ya KTo\n","----------------------------------\n","233/250 (epoch 116), train_loss = 1.729, time/batch = 0.017\n",">> sample mode:\n","Saya nhaAanya teraim kan seya dinya dungilan, Pilik, ye\n","----------------------------------\n","235/250 (epoch 117), train_loss = 1.728, time/batch = 0.017\n",">> sample mode:\n","Saya Sentul undeh jahay Ambtanar sas salah kol Pembulan\n","----------------------------------\n","237/250 (epoch 118), train_loss = 1.727, time/batch = 0.017\n",">> sample mode:\n","Saya samerikaw iplusnhko saya neladanr sapa, beretbihn-\n","----------------------------------\n","239/250 (epoch 119), train_loss = 1.726, time/batch = 0.018\n",">> sample mode:\n","Saya seratak lil PSoAtu Kuk.g Fteda Ja zaengeya bujutun\n","----------------------------------\n","241/250 (epoch 120), train_loss = 1.725, time/batch = 0.018\n",">> sample mode:\n","Saya \n","ing edal herbeha lewanya Benedilis sala\n","----------------------------------\n","243/250 (epoch 121), train_loss = 1.724, time/batch = 0.022\n",">> sample mode:\n","Saya yana gen turan hurat mimeAlndan memekang menggan b\n","----------------------------------\n","245/250 (epoch 122), train_loss = 1.723, time/batch = 0.021\n",">> sample mode:\n","Saya gitiraida Bada serKtan Kes ju2sar dany1 -Pabur(te \n","----------------------------------\n","247/250 (epoch 123), train_loss = 1.723, time/batch = 0.018\n",">> sample mode:\n","Saya semaja, angi menesah PendetakaH dan inglanyaman se\n","----------------------------------\n","249/250 (epoch 124), train_loss = 1.722, time/batch = 0.018\n",">> sample mode:\n","Saya kerdaca nu(temaul kendakar yanya Lama S burum kori\n","----------------------------------\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"lYiw-iu_bNwY","colab_type":"text"},"source":["# Thanks for completing this lesson!\n","Created by: <a href = \"https://linkedin.com/in/saeedaghabozorgi\"> Saeed Aghabozorgi </a></h4>\n","This code is based on this [blog](http://karpathy.github.io/2015/05/21/rnn-effectiveness/), and the code is an step-by-step implimentation of the [character-level implimentation](https://github.com/crazydonkey200/tensorflow-char-rnn)."]},{"cell_type":"markdown","metadata":{"id":"Eyt5VMuIcLho","colab_type":"text"},"source":["<hr>\n","\n","<p>Copyright &copy; 2017 IBM <a href=\"https://cognitiveclass.ai/?utm_source=ML0151&utm_medium=lab&utm_campaign=cclab\">IBM Cognitive Class</a>. This notebook and its source code are released under the terms of the <a href=\"https://cognitiveclass.ai/mit-license/\">MIT License</a>.</p>"]}]}